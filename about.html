<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>People - NerDS Lab</title>
  <link rel="stylesheet" href="style.css">
  <style>
    .research-area-table { width: 100%; margin-bottom: 2em; border-collapse: collapse; }
    .research-area-table td { vertical-align: top; padding: 10px; }
    .research-image { width: 250px; border-radius: 8px; }
    .feature-pubs { margin-top: 10px; }
    h2 { border-bottom: 2px solid #ddd; padding-bottom: 5px; }
  </style>
</head>
<body>
<header>
  <img src="images/logo.png" alt="NerDS Lab Logo" height="80">
  <h1>NerDS Lab</h1>
  <nav>
    <a href="index.html">Home</a> |
    <a href="about.html">About</a> |
    <a href="people.html">People</a> |
    <a href="publications.html">Publications</a> |
    <a href="contact.html">Contact</a>
  </nav>
</header>
<main>

<section id="about" style="padding: 2em 0;">
  <div class="container">
    <h2>About the NerDS Lab</h2>
    <p>The <strong>Neural Data Science (NerDS) Lab</strong>, led by <strong>Dr. Eva Dyer</strong> at UPenn, develops data-centric machine learning methods to understand complex biological systems. Our interdisciplinary research integrates neuroscience, AI, and representation learning to decode neural computation, interpret behavior, and uncover general principles of biological intelligence.</p>

    <table class="research-area-table">
      <tr>
        <td><img class="research-image" src="images/20250523_1818_Neural Network Brainscape_simple_compose_01jvzkjmsqe4ha7x3sbrkcd800.png" alt="Latent space of brain activity"></td>
        <td>
          <h3>üß† Understanding the Brain Through AI</h3>
          <p>We create scalable models that decode brain activity across tasks, cell types, and animals. Our foundation models for neuroscience support transfer across sessions and contexts.</p>
          <div class="feature-pubs">
            <strong>Featured Paper:</strong>
            <ul>
               <li><a href="https://arxiv.org/pdf/2310.16046" style="color: #008b8b;">A unified, scalable framework for neural population decoding (NeurIPS 2023)</a></li>
              <li><a href="https://openreview.net/pdf?id=IuU0wcO0mo" style="color: #008b8b;">Multi-session, multi-task neural decoding from distinct cell-types and brain regions (ICLR 2025)</a></li>
            </ul>
          </div>
        </td>
      </tr>
    </table>

    <table class="research-area-table">
      <tr>
        <td><img class="research-image" src="images/AugmentedCat.png" alt="Behavior embeddings"></td>
        <td>
          <h3>üîÅ Self-Supervised and Contrastive Learning</h3>
          <p>We apply self-supervised methods to reveal hidden patterns in neural and behavioral data. Our models learn structured representations without labels, discovering multiscale behavior and neural motifs.</p>
          <div class="feature-pubs">
            <strong>Featured Paper:</strong>
            <ul>
              <li><a href="https://arxiv.org/abs/2303.08811" style="color: #008b8b;">Relax, it doesn‚Äôt matter how you get there (NeurIPS 2023)</a></li>
            </ul>
          </div>
        </td>
      </tr>
    </table>

    <table class="research-area-table">
      <tr>
        <td><img class="research-image" src="images/20250523_1955_Abstract Manifold Art_simple_compose_01jvzs3dhbe1sbap6d8q90ndhe.png" alt="Domain adaptation visualization"></td>
        <td>
          <h3>üåç Domain Adaptation and Representation Alignment</h3>
          <p>We design representation alignment methods that adapt across devices and conditions in time series data. Our channel-selective and Sinkhorn-based techniques improve model robustness and transfer.</p>
          <div class="feature-pubs">
            <strong>Featured Paper:</strong>
            <ul>
              <li><a href="https://openreview.net/pdf?id=8C8LJIqF4y" style="color: #008b8b;">Time Series Domain Adaptation via Channel-Selective Representation Alignment (TMLR 2025)</a></li>
            </ul>
          </div>
        </td>
      </tr>
    </table>

    <table class="research-area-table">
      <tr>
        <td><img class="research-image" src="images/20250523_1959_Futuristic Neuron Network_simple_compose_01jvzsbj4cesms0r6r6jfrb0w4.png" alt="Cell type classification"></td>
        <td>
          <h3>üß¨ Mapping Cell Types from Activity</h3>
          <p>We build classifiers that use only neural activity to predict cell types and brain regions. Our contrastive approaches generalize across mice and modalities, bridging function and transcriptomics.</p>
          <div class="feature-pubs">
            <strong>Featured Paper:</strong>
            <ul>
              <li><a href="https://www.biorxiv.org/content/10.1101/2024.11.05.622159.abstract" style="color: #008b8b;">In vivo cell-type and brain region classification via multimodal contrastive learning (ICLR 2025)</a></li>
            </ul>
          </div>
        </td>
      </tr>
    </table>

    <table class="research-area-table">
      <tr>
        <td><img class="research-image" src="images/20250523_2009_Graph Nodes Connections_simple_compose_01jvzsxrbzencb72dakj4tgrw6.png" alt="Graph upsampling illustration"></td>
        <td>
          <h3>üìä Graph Learning</h3>
          <p>Our lab advances graph neural network methods to model diverse graph datasets. We develop transformers and graph augmentation techniques for scalable and transferable graph learning.</p>
          <div class="feature-pubs">
            <strong>Featured Paper:</strong>
            <ul>
              <li><a href="http://proceedings.mlr.press/v202/azabou23a/azabou23a.pdf" style="color: #008b8b;">Half-Hop: A graph upsampling approach for slowing down message passing (ICML 2023)</a></li>
            </ul>
          </div>
        </td>
      </tr>
    </table>
  </div>
</section>
</main>
</body>
</html>
