
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>People - NerDS Lab</title>
  <link rel="stylesheet" href="style.css">
  <style>
    .research-area-table { width: 100%; margin-bottom: 2em; border-collapse: collapse; }
    .research-area-table td { vertical-align: top; padding: 10px; }
    .research-image { width: 200px; border-radius: 8px; }
    .feature-pubs { margin-top: 10px; }
    h2 { border-bottom: 2px solid #ddd; padding-bottom: 5px; }
  </style>
</head>
<body>
<header>
  <img src="images/logo_outline.png" alt="NerDS Lab Logo" height="80">
  <h1>NerDS Lab</h1>
  <nav>
    <a href="index.html">Home</a> |
    <a href="about.html">About</a> |
    <a href="people.html">People</a> |
    <a href="publications.html">Publications</a> |
    <a href="contact.html">Contact</a>
  </nav>
</header>
<main>

<section id="about" style="padding: 2em 0;">
  <div class="container">
    <h2>About the NerDS Lab</h2>
    <p>The <strong>Neural Data Science (NerDS) Lab</strong>, led by <strong>Dr. Eva Dyer</strong> at UPenn, develops data-centric machine learning methods to understand complex biological systems. Our interdisciplinary research integrates neuroscience, AI, and representation learning to decode neural computation, interpret behavior, and uncover general principles of biological intelligence.</p>

    <table class="research-area-table">
      <tr>
        <td><img class="research-image" src="images/20250523_1818_Neural Network Brainscape_simple_compose_01jvzkjmsqe4ha7x3sbrkcd800.png" alt="Latent space of brain activity"></td>
        <td>
          <h3>üß† Understanding the Brain Through AI</h3>
          <p>We create scalable models that decode brain activity across tasks, cell types, and animals. Our foundation models for neuroscience support transfer across sessions and contexts.</p>
          <div class="feature-pubs">
            <strong>Featured Paper:</strong>
            <ul>
              <li><a href="https://arxiv.org/pdf/2310.16046" style="color: #008b8b;">A unified, scalable framework for neural population decoding (NeurIPS 2023)</a></li>
              <li><a href="https://openreview.net/pdf?id=IuU0wcO0mo" style="color: #008b8b;">Multi-session, multi-task neural decoding from distinct cell-types and brain regions (ICLR 2025)</a></li>
            </ul>
          </div>
        </td>
      </tr>
    </table>

    <table class="research-area-table">
      <tr>
        <td><img class="research-image" src="images/AugmentedCat.png" alt="Behavior embeddings"></td>
        <td>
          <h3>üîÅ Self-Supervised and Contrastive Learning</h3>
          <p>We apply self-supervised methods to reveal hidden patterns in neural and behavioral data. Our models learn structured representations without labels, discovering multiscale behavior and neural motifs.</p>
          <div class="feature-pubs">
            <strong>Featured Paper:</strong>
            <ul>
              <li><a href="https://arxiv.org/abs/2303.08811" style="color: #008b8b;">Relax, it doesn‚Äôt matter how you get there (NeurIPS 2023)</a></li>
              <li><a href="https://arxiv.org/pdf/2111.02338" style="color: #008b8b;">Drop, Swap, and Generate (NeurIPS 2021, Oral)</a></li>
            </ul>
          </div>
        </td>
      </tr>
    </table>

    <table class="research-area-table">
      <tr>
        <td><img class="research-image" src="images/20250523_2016_Abstract Manifold Art_simple_compose_01jvztaye3e4ytz8tvbzvjnq9x.png" alt="Domain adaptation visualization"></td>
        <td>
          <h3>üåç Domain Adaptation and Representation Alignment</h3>
          <p>We design representation alignment methods that adapt across devices and conditions in time series data. Our channel-selective and Sinkhorn-based techniques improve model robustness and transfer.</p>
          <div class="feature-pubs">
            <strong>Featured Paper:</strong>
            <ul>
              <li><a href="https://openreview.net/pdf?id=8C8LJIqF4y" style="color: #008b8b;">Channel-Selective Representation Alignment (TMLR 2025)</a></li>
              <li><a href="https://proceedings.mlr.press/v139/lin21a.html" style="color: #008b8b;">Moving data through anchor points (ICML 2021)</a></li>
            </ul>
          </div>
        </td>
      </tr>
    </table>

    <table class="research-area-table">
      <tr>
        <td><img class="research-image" src="images/20250523_1959_Futuristic Neuron Network_simple_compose_01jvzsbj4cesms0r6r6jfrb0w4.png" alt="Cell type classification"></td>
        <td>
          <h3>üß¨ Mapping Cell Types from Activity</h3>
          <p>We build classifiers that use only neural activity to predict cell types and brain regions. Our contrastive approaches generalize across mice and modalities, bridging function and transcriptomics.</p>
          <div class="feature-pubs">
            <strong>Featured Paper:</strong>
            <ul>
              <li><a href="https://www.biorxiv.org/content/10.1101/2024.11.05.622159.abstract" style="color: #008b8b;">Multimodal contrastive learning (ICLR 2025)</a></li>
              <li><a href="https://www.cell.com/cell-reports/pdfExtended/S2211-1247(23)00329-7" style="color: #008b8b;">Transcriptomic cell type structures (Cell Reports 2023)</a></li>
            </ul>
          </div>
        </td>
      </tr>
    </table>

    <table class="research-area-table">
      <tr>
        <td><img class="research-image" src="images/20250523_2009_Graph Nodes Connections_simple_compose_01jvzsxrbzencb72dakj4tgrw6.png" alt="Graph upsampling illustration"></td>
        <td>
          <h3>üìä Graph Learning</h3>
          <p>Our lab advances graph neural network methods to model diverse graph datasets. We develop transformers and graph augmentation techniques for scalable and transferable graph learning.</p>
          <div class="feature-pubs">
            <strong>Featured Paper:</strong>
            <ul>
              <li><a href="http://proceedings.mlr.press/v202/azabou23a/azabou23a.pdf" style="color: #008b8b;">Half-Hop (ICML 2023)</a></li>
              <li><a href="https://arxiv.org/abs/2407.11907" style="color: #008b8b;">GraphFM (arXiv 2024)</a></li>
            </ul>
          </div>
        </td>
      </tr>
    </table>

  </div>
</section>
</main>
</body>
</html>
